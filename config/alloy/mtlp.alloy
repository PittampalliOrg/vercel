# 'config/alloy/config.alloy'
livedebugging {
  enabled = true
}

discovery.relabel "node_exporter" {
  targets = [{
    __address__ = "node-exporter:9100",  // Adjust if your Node Exporter is at a different address
  }]

  rule {
    target_label = "instance"
    replacement  = constants.hostname
  }

  rule {
    target_label = "job"
    replacement  = "node"  // Changed to "node" to match standard dashboards
  }
}

prometheus.scrape "node_exporter" {
  targets         = discovery.relabel.node_exporter.output
  forward_to      = [otelcol.receiver.prometheus.node.receiver]
  job_name        = "node"  // Consistent with the relabel rule
  scrape_interval = "15s"
}

otelcol.receiver.prometheus "node" {
  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}

otelcol.receiver.filelog "postgres_logs" {
  include = ["/var/lib/docker/containers/*/*.log"]
  operators = [
    {
      // Router to identify Docker JSON logs
      type = "router",
      id   = "get-format",
      routes = [
        {
          output = "extract_docker_metadata",
          expr   = "body matches \"^\\\\{\"",
        },
      ],
    },
    {
      // Extract Docker metadata
      type   = "json_parser",
      id     = "extract_docker_metadata",
      output = "filter_postgres",
    },
    {
      // Filter to only include PostgreSQL container logs
      type   = "filter",
      id     = "filter_postgres",
      expr   = "attributes.container_name matches \".*postgres.*\"",
      output = "extract_postgres_log",
    },
    {
      // Parse PostgreSQL logs
      type   = "json_parser",
      id     = "extract_postgres_log",
      field  = "attributes.log",
      output = "add_resource_attributes",
    },
    {
      // Add resource attributes for correlation
      type  = "add",
      id    = "add_resource_attributes",
      field = "resource.attributes.service.name",
      value = "postgresql",
    },
  ]
  output {
    logs = [otelcol.exporter.debug.default.input]
  }
}

otelcol.exporter.debug "default" {}

otelcol.receiver.otlp "default" {
  // https://grafana.com/docs/alloy/latest/reference/components/otelcol.receiver.otlp/
  
  grpc { }  // default gRPC endpoint "0.0.0.0:4317"
  http {
    cors {
      allowed_origins = ["http://localhost", "https://*"]
      allowed_headers = ["Content-Type", "X-Api-Key", "X-Faro-Session-Id"]
      max_age         = 7200
    }
    include_metadata = true
  }
  
  output {
    metrics = [
      otelcol.exporter.debug.traefik_metrics.input,
      otelcol.processor.resourcedetection.default.input
    ]
    traces  = [
      otelcol.exporter.debug.traefik_traces.input,
      otelcol.processor.resourcedetection.default.input
      // Note: We do NOT directly send traces to autologging here.
      // We'll do that after the batch processor so the spans are fully processed.
    ]
    logs    = [otelcol.processor.resourcedetection.default.input]
  }
}

otelcol.processor.resourcedetection "default" {
  // https://grafana.com/docs/alloy/latest/reference/components/otelcol.processor.resourcedetection/
  detectors = ["env", "system"]
  
  system {
    hostname_sources = ["os"]
  }
  
  output {
    metrics = [otelcol.processor.transform.drop_unneeded_resource_attributes.input]
    logs    = [otelcol.processor.transform.drop_unneeded_resource_attributes.input]
    traces  = [otelcol.processor.transform.drop_unneeded_resource_attributes.input]
  }
}

otelcol.processor.transform "drop_unneeded_resource_attributes" {
  // https://grafana.com/docs/alloy/latest/reference/components/otelcol.processor.transform/
  error_mode = "ignore"
  
  trace_statements {
    context    = "resource"
    statements = [
      "delete_key(attributes, \"k8s.pod.start_time\")",
      "delete_key(attributes, \"os.description\")",
      "delete_key(attributes, \"os.type\")",
      "delete_key(attributes, \"process.command_args\")",
      "delete_key(attributes, \"process.executable.path\")",
      "delete_key(attributes, \"process.pid\")",
      "delete_key(attributes, \"process.runtime.description\")",
      "delete_key(attributes, \"process.runtime.name\")",
      "delete_key(attributes, \"process.runtime.version\")",
    ]
  }
  
  metric_statements {
    context    = "resource"
    statements = [
      "delete_key(attributes, \"k8s.pod.start_time\")",
      "delete_key(attributes, \"os.description\")",
      "delete_key(attributes, \"os.type\")",
      "delete_key(attributes, \"process.command_args\")",
      "delete_key(attributes, \"process.executable.path\")",
      "delete_key(attributes, \"process.pid\")",
      "delete_key(attributes, \"process.runtime.description\")",
      "delete_key(attributes, \"process.runtime.name\")",
      "delete_key(attributes, \"process.runtime.version\")",
    ]
  }
  
  log_statements {
    context    = "resource"
    statements = [
      "delete_key(attributes, \"k8s.pod.start_time\")",
      "delete_key(attributes, \"os.description\")",
      "delete_key(attributes, \"os.type\")",
      "delete_key(attributes, \"process.command_args\")",
      "delete_key(attributes, \"process.executable.path\")",
      "delete_key(attributes, \"process.pid\")",
      "delete_key(attributes, \"process.runtime.description\")",
      "delete_key(attributes, \"process.runtime.name\")",
      "delete_key(attributes, \"process.runtime.version\")",
    ]
  }
  
  output {
    metrics = [otelcol.processor.transform.add_resource_attributes_as_metric_attributes.input]
    logs    = [otelcol.processor.batch.default.input]
    traces  = [
      otelcol.connector.servicegraph.default.input,
      otelcol.connector.spanmetrics.default.input,
      otelcol.processor.probabilistic_sampler.default.input,
      otelcol.connector.host_info.default.input,
    ]
  }
}

otelcol.connector.servicegraph "default" {
  // https://grafana.com/docs/alloy/latest/reference/components/otelcol.connector.servicegraph/
  dimensions = [
    "service.namespace",
    "service.version",
    "deployment.environment",
    "k8s.cluster.name",
    "k8s.namespace.name",
    "cloud.region",
    "cloud.availability_zone",
  ]
  latency_histogram_buckets = [
    "0.005s", "0.01s", "0.025s", "0.05s", "0.075s", "0.1s",
    "0.25s", "0.5s", "0.75s", "1s", "2.5s", "5s", "7.5s", "10s"
  ]
  
  store {
    ttl = "10s"
  }
  
  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}

otelcol.connector.spanmetrics "default" {
  // https://grafana.com/docs/alloy/latest/reference/components/otelcol.connector.spanmetrics/
  dimension {
    name = "service.namespace"
  }
  dimension {
    name = "service.version"
  }
  dimension {
    name = "deployment.environment"
  }
  dimension {
    name = "k8s.cluster.name"
  }
  dimension {
    name = "k8s.namespace.name"
  }
  dimension {
    name = "cloud.region"
  }
  dimension {
    name = "cloud.availability_zone"
  }
  
  histogram {
    explicit {
      buckets = [
        "0.005s", "0.01s", "0.025s", "0.05s", "0.075s",
        "0.1s", "0.25s", "0.5s", "0.75s", "1s",
        "2.5s", "5s", "7.5s", "10s"
      ]
    }
    unit = "s"
  }

  # ADDED/UPDATED: Enable exemplars for metric→trace correlation
  exemplars {
    enabled = true
  }

  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}

otelcol.processor.probabilistic_sampler "default" {
  // https://grafana.com/docs/alloy/latest/reference/components/otelcol.processor.probabilistic_sampler/
  sampling_percentage = 100 // Currently keeping 100% of traces

  output {
    traces = [otelcol.processor.batch.default.input]
  }
}

otelcol.connector.host_info "default" {
  // https://grafana.com/docs/alloy/latest/reference/components/otelcol.connector.host_info/
  host_identifiers = ["host.name"]
  
  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}

otelcol.processor.transform "add_resource_attributes_as_metric_attributes" {
  // https://grafana.com/docs/alloy/latest/reference/components/otelcol.processor.transform/
  error_mode = "ignore"
  
  metric_statements {
    context    = "datapoint"
    statements = [
      "set(attributes[\"deployment.environment\"], resource.attributes[\"deployment.environment\"])",
      "set(attributes[\"service.version\"], resource.attributes[\"service.version\"])",
    ]
  }
  
  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}

otelcol.processor.batch "default" {
  // https://grafana.com/docs/alloy/latest/reference/components/otelcol.processor.batch/
  output {
    metrics = [otelcol.exporter.otlphttp.default_mimir.input]
    logs    = [otelcol.exporter.otlphttp.default_loki.input]

    # UPDATED: Also send traces to the new autologging pipeline for trace→logs correlation
    traces   = [
      otelcol.exporter.otlp.default_tempo.input,
      otelcol.connector.spanlogs.autologging.input
    ]
  }
}

otelcol.exporter.otlphttp "default_mimir" {
  client {
    endpoint                = "http://mimir:9009/otlp"
    max_idle_conns_per_host = 0
    max_conns_per_host      = 0
    http2_ping_timeout      = "0s"
  }
}

otelcol.exporter.otlphttp "default_loki" {
  client {
    endpoint                = "http://loki:3100/otlp"
    max_idle_conns_per_host = 0
    max_conns_per_host      = 0
    http2_ping_timeout      = "0s"
  }
}

otelcol.exporter.otlp "default_tempo" {
  client {
    endpoint = "tempo:4317"
    tls {
      insecure = true
    }
  }
}

// Process discovery for profiling
discovery.process "all" { }

discovery.relabel "alloy" {
  targets = discovery.process.all.targets
  // Filter needed processes
  rule {
    source_labels = ["__meta_process_exe"]
    regex         = ".*/alloy"
    action        = "keep"
  }
  
  // provide arbitrary service_name label, otherwise it will be "unspecified"
  rule {
    source_labels = ["__meta_process_exe"]
    target_label  = "service_name"
    regex         = ".*/alloy"
    action        = "replace"
    replacement   = "ebpf/local/alloy"
  }
}

prometheus.exporter.postgres "postgres_table_size" {
  data_source_names           = ["postgresql://postgres:postgres@db:5432/postgres?sslmode=disable"]
  custom_queries_config_path  = "/etc/alloy/pg_table_size-details.yaml"
}

prometheus.exporter.postgres "postgres_queries" {
  data_source_names           = ["postgresql://postgres:postgres@db:5432/postgres?sslmode=disable"]
  custom_queries_config_path  = "/etc/alloy/queries-pg_stat.yaml"
  disable_settings_metrics    = true
}

prometheus.scrape "postgres_query_metrics" {
  targets    = prometheus.exporter.postgres.postgres_queries.targets
  forward_to = [prometheus.relabel.truncate_query_labels.receiver]
}

prometheus.relabel "truncate_query_labels" {
  rule {
    source_labels = ["query"]
    regex         = "(.{1,1000}).*"
    target_label  = "query"
    replacement   = "$1..."
  }
  
  forward_to = [otelcol.receiver.prometheus.postgres.receiver]
}

prometheus.scrape "postgres_table_size_metrics" {
  targets    = prometheus.exporter.postgres.postgres_table_size.targets
  forward_to = [otelcol.receiver.prometheus.postgres.receiver]
}

otelcol.receiver.prometheus "postgres" {
  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}

// PostgreSQL log collection
local.file_match "logs_integrations_postgres_exporter" {
  path_targets = [{
    __address__ = "localhost",
    __path__    = "/logs/postgresql-*.log",
    instance    = constants.hostname,
    job         = "integrations/postgres_exporter",
  }]
}

loki.source.file "logs_integrations_postgres_exporter" {
  targets    = local.file_match.logs_integrations_postgres_exporter.targets
  forward_to = [otelcol.receiver.loki.postgres_logs.receiver]
}

otelcol.receiver.loki "postgres_logs" {
  output {
    logs = [otelcol.processor.batch.default.input]
  }
}

discovery.relabel "metrics_integrations_integrations_traefik" {
  targets = [{
    __address__ = "traefik:8080",
  }]

  rule {
    target_label = "instance"
    replacement  = constants.hostname
  }
}

prometheus.scrape "metrics_integrations_integrations_traefik" {
  targets    = discovery.relabel.metrics_integrations_integrations_traefik.output
  forward_to = [prometheus.remote_write.default.receiver]
  job_name   = "integrations/traefik"
}

// Option 1: If you want to use Prometheus format
prometheus.remote_write "default" {
  endpoint {
    url = "http://mimir:9009/api/prom/push"  // Adjust to your endpoint
  }
}
otelcol.exporter.debug "traefik_metrics" {
  verbosity = "detailed"
}

otelcol.exporter.debug "traefik_traces" {
  verbosity = "detailed"
}

discovery.relabel "cadvisor" {
  targets = [{
    __address__ = "cadvisor:8080",
  }]

  rule {
    target_label = "instance"
    replacement  = constants.hostname
  }

  rule {
    target_label = "job"
    replacement  = "cadvisor"
  }
}

prometheus.scrape "cadvisor" {
  targets         = discovery.relabel.cadvisor.output
  forward_to      = [otelcol.receiver.prometheus.cadvisor.receiver]
  job_name        = "cadvisor"
  scrape_interval = "15s"
}

otelcol.receiver.prometheus "cadvisor" {
  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}

###############################################################################
# ADDED: Span → Log connector to generate logs from root spans, plus
#        a small Loki pipeline that ships them to your "loki:3100" endpoint.
###############################################################################

otelcol.connector.spanlogs "autologging" {
  # Only turn root spans into logs:
  roots     = true
  spans     = false
  processes = false

  # Include whichever span attributes you'd like in the log line
  span_attributes = [
    "http.method",
    "http.url",
    "http.route",
    "http.status_code",
    "db.statement",
  ]

  overrides {
    # By default, the field is 'traceID' for correlation in Loki
    trace_id_key = "traceID"
  }

  output {
    logs = [otelcol.exporter.loki.autologging_spanlogs.input]
  }
}

otelcol.exporter.loki "autologging_spanlogs" {
  # Convert OTel logs → standard Loki log streams
  forward_to = [loki.process.autologging_spanlogs.receiver]
}

loki.process "autologging_spanlogs" {
  # Example pipeline: parse JSON "body" if needed
  stage.json {
    expressions = { "body" = "" }
  }
  stage.output {
    source = "body"
  }

  forward_to = [loki.write.autologging_spanlogs.receiver]
}

loki.write "autologging_spanlogs" {
  external_labels = {
    job = "spanlogs"
  }

  endpoint {
    url = "http://loki:3100/loki/api/v1/push"
    # If auth is required, uncomment:
    # basic_auth {
    #   username = "..."
    #   password = "..."
    # }
  }
}
