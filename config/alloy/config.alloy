// === GLOBAL SETTINGS ===
livedebugging {
  enabled = true
}

// === POSTGRESQL MONITORING ===
prometheus.exporter.postgres "integrations_postgres_exporter" {
  data_source_names = ["postgresql://postgres:postgres@db:5432/postgres?sslmode=disable"]
  disable_default_metrics = false

  autodiscovery {
    enabled = true
  }
}

discovery.relabel "integrations_postgres_exporter" {
  targets = prometheus.exporter.postgres.integrations_postgres_exporter.targets
  
  rule {
    target_label = "instance"
    replacement  = constants.hostname
  }
  rule {
    target_label = "job"
    replacement  = "integrations/postgres_exporter"
  }
}

prometheus.scrape "integrations_postgres_exporter" {
  targets    = discovery.relabel.integrations_postgres_exporter.output
  forward_to = [prometheus.remote_write.default.receiver]
  job_name   = "integrations/postgres_exporter"
  scrape_interval = "15s"
}

// === POSTGRESQL LOG COLLECTION ===
// Unified approach for both Docker and traditional logs
local.file_match "logs_integrations_postgres_exporter" {
  path_targets = [{
    __address__ = "localhost",
    __path__    = "/var/log/postgresql/*.log,/logs/postgresql*.log",
    instance    = constants.hostname,
    job         = "integrations/postgres_exporter",
  }]
}

loki.source.file "logs_integrations_postgres_exporter" {
  targets    = local.file_match.logs_integrations_postgres_exporter.targets
  forward_to = [loki.write.default.receiver]
}

// === NODE EXPORTER ===
discovery.relabel "node_exporter" {
  targets = [{
    __address__ = "node-exporter:9100",
  }]
  
  rule {
    target_label = "instance"
    replacement  = constants.hostname
  }
  
  rule {
    target_label = "job"
    replacement  = "node"
  }
}

prometheus.scrape "node_exporter" {
  targets         = discovery.relabel.node_exporter.output
  forward_to      = [prometheus.remote_write.default.receiver]
  job_name        = "node"
  scrape_interval = "15s"
}

// === TRAEFIK MONITORING ===
discovery.relabel "metrics_integrations_integrations_traefik" {
  targets = [{
    __address__ = "traefik:8080",
  }]
  
  rule {
    target_label = "instance"
    replacement  = constants.hostname
  }
}

prometheus.scrape "metrics_integrations_integrations_traefik" {
  targets    = discovery.relabel.metrics_integrations_integrations_traefik.output
  forward_to = [prometheus.remote_write.default.receiver]
  job_name   = "integrations/traefik"
}

// === CADVISOR MONITORING ===
discovery.relabel "cadvisor" {
  targets = [{
    __address__ = "cadvisor:8080",
  }]
  
  rule {
    target_label = "instance"
    replacement  = constants.hostname
  }
  
  rule {
    target_label = "job"
    replacement  = "cadvisor"
  }
}

prometheus.scrape "cadvisor" {
  targets         = discovery.relabel.cadvisor.output
  forward_to      = [prometheus.remote_write.default.receiver]
  job_name        = "cadvisor"
  scrape_interval = "15s"
}

// === OTLP RECEIVER ===
otelcol.receiver.otlp "default" {
  grpc {
    endpoint = "0.0.0.0:4317"
  }
  http {
    endpoint = "0.0.0.0:4318"
    cors {
      allowed_origins = ["http://localhost", "https://*"]
      allowed_headers = ["Content-Type", "X-Api-Key", "X-Faro-Session-Id"]
      max_age         = 7200
    }
    include_metadata = true
  }
  
  output {
    metrics = [otelcol.processor.resourcedetection.default.input]
    traces  = [otelcol.processor.resourcedetection.default.input]
    logs    = [otelcol.processor.resourcedetection.default.input]
  }
}

// === PROCESSING PIPELINE ===
otelcol.processor.resourcedetection "default" {
  detectors = ["env", "system"]
  
  system {
    hostname_sources = ["os"]
  }
  
  output {
    metrics = [otelcol.processor.transform.drop_unneeded_resource_attributes.input]
    logs    = [otelcol.processor.transform.drop_unneeded_resource_attributes.input]
    traces  = [otelcol.processor.transform.drop_unneeded_resource_attributes.input]
  }
}

otelcol.processor.transform "drop_unneeded_resource_attributes" {
  error_mode = "ignore"
  
  trace_statements {
    context = "resource"
    statements = [
      "delete_key(resource.attributes, \"k8s.pod.start_time\")",
      "delete_key(resource.attributes, \"os.description\")",
      "delete_key(resource.attributes, \"os.type\")",
      "delete_key(resource.attributes, \"process.command_args\")",
      "delete_key(resource.attributes, \"process.executable.path\")",
      "delete_key(resource.attributes, \"process.pid\")",
      "delete_key(resource.attributes, \"process.runtime.description\")",
      "delete_key(resource.attributes, \"process.runtime.name\")",
      "delete_key(resource.attributes, \"process.runtime.version\")",
    ]
  }
  
  metric_statements {
    context = "resource"
    statements = [
      "delete_key(resource.attributes, \"k8s.pod.start_time\")",
      "delete_key(resource.attributes, \"os.description\")",
      "delete_key(resource.attributes, \"os.type\")",
      "delete_key(resource.attributes, \"process.command_args\")",
      "delete_key(resource.attributes, \"process.executable.path\")",
      "delete_key(resource.attributes, \"process.pid\")",
      "delete_key(resource.attributes, \"process.runtime.description\")",
      "delete_key(resource.attributes, \"process.runtime.name\")",
      "delete_key(resource.attributes, \"process.runtime.version\")",
    ]
  }
  
  log_statements {
    context = "resource"
    statements = [
      "delete_key(resource.attributes, \"k8s.pod.start_time\")",
      "delete_key(resource.attributes, \"os.description\")",
      "delete_key(resource.attributes, \"os.type\")",
      "delete_key(resource.attributes, \"process.command_args\")",
      "delete_key(resource.attributes, \"process.executable.path\")",
      "delete_key(resource.attributes, \"process.pid\")",
      "delete_key(resource.attributes, \"process.runtime.description\")",
      "delete_key(resource.attributes, \"process.runtime.name\")",
      "delete_key(resource.attributes, \"process.runtime.version\")",
    ]
  }
  
  output {
    metrics = [otelcol.processor.transform.add_resource_attributes_as_metric_attributes.input]
    logs = [otelcol.processor.batch.default.input]
    traces = [
      otelcol.connector.servicegraph.default.input,
      otelcol.connector.spanmetrics.default.input,
      otelcol.processor.probabilistic_sampler.default.input,
      otelcol.connector.host_info.default.input,
    ]
  }
}

otelcol.connector.servicegraph "default" {
  dimensions = [
    "service.namespace",
    "service.name",
    "service.version",
  ]
  
  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}

otelcol.connector.spanmetrics "default" {
  dimension {
    name = "service.name"
  }
  histogram {
    explicit {
      buckets = [
        "0.005s", "0.01s", "0.025s", "0.05s", "0.075s",
        "0.1s", "0.25s", "0.5s", "0.75s", "1s",
        "2.5s", "5s", "7.5s", "10s",
      ]
    }
    unit = "s"
  }
  
  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}

otelcol.processor.probabilistic_sampler "default" {
  sampling_percentage = 100
  
  output {
    traces = [otelcol.processor.batch.default.input]
  }
}

otelcol.connector.host_info "default" {
  host_identifiers = ["host.name"]
  
  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}

otelcol.processor.transform "add_resource_attributes_as_metric_attributes" {
  error_mode = "ignore"
  
  metric_statements {
    context = "datapoint"
    statements = [
      "set(attributes[\"deployment.environment\"], resource.attributes[\"deployment.environment\"])",
      "set(attributes[\"service.version\"], resource.attributes[\"service.version\"])",
    ]
  }
  
  output {
    metrics = [otelcol.processor.batch.default.input]
  }
}

otelcol.connector.spanlogs "autologging" {
  roots = true
  
  output {
    logs = [otelcol.exporter.loki.autologging_spanlogs.input]
  }
}

otelcol.exporter.loki "autologging_spanlogs" {
  forward_to = [loki.write.default.receiver]
}

otelcol.processor.batch "default" {
  timeout = "1s"
  send_batch_size = 1024
  send_batch_max_size = 2048
  
  output {
    metrics = [otelcol.exporter.otlphttp.default_mimir.input]
    logs    = [otelcol.exporter.otlphttp.default_loki.input]
    traces  = [
      otelcol.exporter.otlp.default_tempo.input,
      otelcol.connector.spanlogs.autologging.input,
    ]
  }
}

// === EXPORTERS WITH IMPROVED ERROR HANDLING ===
otelcol.exporter.otlphttp "default_mimir" {
  client {
    endpoint = "http://mimir:9009/otlp"
    // REMOVE: timeout = "5s"
  }
}

otelcol.exporter.otlphttp "default_loki" {
  client {
    endpoint = "http://loki:3100/otlp"
    // REMOVE: timeout = "5s"
  }
}

// === REMOTE WRITE ENDPOINTS ===
prometheus.remote_write "default" {
  endpoint {
    url = "http://mimir:9009/api/prom/push"
    send_exemplars = true
    
    // Move queue_config inside the endpoint block
    queue_config {
      capacity = 10000
      max_samples_per_send = 2000
      batch_send_deadline = "5s"
      min_backoff = "500ms"
      max_backoff = "5s"
      retry_on_http_429 = true
    }
  }
}

loki.write "default" {
  endpoint {
    url = "http://loki:3100/loki/api/v1/push"
  }
  external_labels = {
    job = "alloy_logs",
  }
}

otelcol.exporter.otlp "default_tempo" {
  client {
    endpoint = "tempo:4317"
    tls {
      insecure = true
    }
    // REMOVE: timeout = "5s"
    
    // Optionally add these if needed:
    // write_buffer_size = 512 * 1024  // 512 KiB
    // read_buffer_size = 512 * 1024   // 512 KiB
  }
}